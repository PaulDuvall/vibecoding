---
# Refactoring Rules & Code Smells - Enhanced Edition

This file is the **canonical refactoring rulebook** for this project. It catalogs code smells and refactoring techniques based on Martin Fowler's Refactoring Catalog and project-specific conventions. All contributors must follow these guidelines for code quality and maintainability.

For enforcement and integration details, see `.windsurfrules.md`.

---
## Code Smells
---

### Bloaters
*Code, data, or responsibilities have grown excessively large and are hard to work with.*

---

## Smell: Long Method

**ID:** `smell_long_method`
**Severity:** High
**Description:** A method, function, or procedure that has grown too long, making it difficult to understand, maintain, and reuse.

**Detection Patterns:**
- **Metrics:** Method length > 20 lines (excluding docstrings)
- **Complexity:** Cyclomatic complexity > 10
- **AST Pattern:** Function with > 5 control flow statements
- **Tools:** 
  ```bash
  # Flake8: C901 (complexity)
  # Pylint: R0915 (too-many-statements)
  # Radon: cc -nc path/to/file.py
  ```

**Code Example (Before):**
```python
def process_digest_items(items, config):
    """Process and prepare digest items for email."""
    processed_items = []
    total_items = 0
    failed_items = 0
    
    for item in items:
        if not item.get('title'):
            failed_items += 1
            continue
            
        # Validate item
        if not item.get('link') or not item.get('published'):
            failed_items += 1
            continue
            
        # Parse date
        try:
            pub_date = datetime.strptime(item['published'], '%Y-%m-%d')
        except ValueError:
            try:
                pub_date = datetime.strptime(item['published'], '%Y-%m-%dT%H:%M:%S')
            except ValueError:
                failed_items += 1
                continue
        
        # Check if item is recent
        days_old = (datetime.now() - pub_date).days
        if days_old > config.get('max_age_days', 7):
            continue
            
        # Summarize content
        summary = None
        if item.get('content'):
            if len(item['content']) > 500:
                summary = item['content'][:500] + '...'
            else:
                summary = item['content']
        
        # Format item
        formatted_item = {
            'title': item['title'].strip(),
            'link': item['link'].strip(),
            'published': pub_date.strftime('%B %d, %Y'),
            'summary': summary or 'No summary available',
            'source': item.get('source', 'Unknown')
        }
        
        processed_items.append(formatted_item)
        total_items += 1
    
    # Log results
    print(f"Processed {total_items} items, {failed_items} failed")
    
    return processed_items
```

**Code Example (After):**
```python
def process_digest_items(items: List[Dict], config: Dict) -> List[Dict]:
    """Process and prepare digest items for email."""
    processor = DigestItemProcessor(config)
    return processor.process_items(items)


class DigestItemProcessor:
    def __init__(self, config: Dict):
        self.config = config
        self.processed_count = 0
        self.failed_count = 0
    
    def process_items(self, items: List[Dict]) -> List[Dict]:
        processed_items = []
        for item in items:
            if processed_item := self._process_single_item(item):
                processed_items.append(processed_item)
                self.processed_count += 1
            else:
                self.failed_count += 1
        
        self._log_results()
        return processed_items
    
    def _process_single_item(self, item: Dict) -> Optional[Dict]:
        if not self._is_valid_item(item):
            return None
            
        pub_date = self._parse_date(item.get('published'))
        if not pub_date or not self._is_recent(pub_date):
            return None
            
        return self._format_item(item, pub_date)
    
    def _is_valid_item(self, item: Dict) -> bool:
        return all([
            item.get('title'),
            item.get('link'),
            item.get('published')
        ])
    
    def _parse_date(self, date_str: str) -> Optional[datetime]:
        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S']:
            try:
                return datetime.strptime(date_str, fmt)
            except ValueError:
                continue
        return None
    
    def _is_recent(self, pub_date: datetime) -> bool:
        days_old = (datetime.now() - pub_date).days
        return days_old <= self.config.get('max_age_days', 7)
    
    def _format_item(self, item: Dict, pub_date: datetime) -> Dict:
        return {
            'title': item['title'].strip(),
            'link': item['link'].strip(),
            'published': pub_date.strftime('%B %d, %Y'),
            'summary': self._get_summary(item.get('content')),
            'source': item.get('source', 'Unknown')
        }
    
    def _get_summary(self, content: Optional[str]) -> str:
        if not content:
            return 'No summary available'
        return content[:500] + '...' if len(content) > 500 else content
    
    def _log_results(self):
        print(f"Processed {self.processed_count} items, {self.failed_count} failed")
```

**Potential Problems:**
* Hard to understand, debug, and change.
* Promotes code duplication.
* Low cohesion; often mixes multiple responsibilities.
* Difficult to reuse parts of the logic.
* Increased likelihood of subtle errors.

**Refactorings (Solutions):**
* **Extract Method:** (See Rule: `ref_extract_method`)
* **Replace Temp with Query:** (See Rule: `ref_replace_temp_with_query`)
* **Decompose Conditional:** (See Rule: `ref_decompose_conditional`)
* **Introduce Parameter Object:** (See Rule: `ref_introduce_parameter_object`)
* **Replace Method with Method Object:** (See Rule: `ref_replace_method_with_method_object`)
* **Preserve Whole Object:** (See Rule: `ref_preserve_whole_object`)

**Tags:** `#bloaters`, `#comprehensibility`, `#maintenance`

---

## Smell: Large Class

**ID:** `smell_large_class`
**Severity:** High
**Description:** A class that is trying to do too much, often indicated by numerous instance variables, too many methods, or a very long source file. Violates the Single Responsibility Principle.

**Detection Patterns:**
- **Metrics:** 
  - Class lines > 250
  - Number of methods > 20
  - Number of instance variables > 10
- **AST Pattern:** Class with multiple unrelated method groups
- **Tools:**
  ```bash
  # Pylint: R0902 (too-many-instance-attributes)
  # Pylint: R0904 (too-many-public-methods)
  # wc -l to check file length
  ```

**Code Example (Before):**
```python
class EmailDigestService:
    """Handles everything related to email digests."""
    
    def __init__(self):
        # RSS Feed attributes
        self.feed_urls = []
        self.feed_timeout = 30
        self.feed_cache = {}
        
        # Email attributes
        self.smtp_host = os.getenv('SMTP_HOST')
        self.smtp_port = 587
        self.from_email = os.getenv('FROM_EMAIL')
        self.to_emails = []
        
        # Content processing attributes
        self.openai_client = OpenAI()
        self.max_summary_length = 200
        self.content_filters = []
        
        # Scheduling attributes
        self.schedule_time = "09:00"
        self.timezone = "UTC"
        self.last_run = None
        
    # RSS Feed methods
    def add_feed(self, url): ...
    def remove_feed(self, url): ...
    def fetch_feeds(self): ...
    def parse_feed(self, content): ...
    def cache_feed(self, url, content): ...
    
    # Email methods
    def send_email(self, content): ...
    def format_html_email(self, items): ...
    def validate_email_config(self): ...
    def add_recipient(self, email): ...
    
    # Content processing methods
    def summarize_content(self, text): ...
    def filter_content(self, items): ...
    def rank_items(self, items): ...
    def extract_keywords(self, text): ...
    
    # Scheduling methods
    def schedule_digest(self): ...
    def should_run_now(self): ...
    def update_last_run(self): ...
```

**Code Example (After):**
```python
# Separated into focused classes
class FeedManager:
    """Manages RSS feed sources."""
    def __init__(self):
        self.feed_urls = []
        self.timeout = 30
        self.cache = {}
    
    def add_feed(self, url): ...
    def remove_feed(self, url): ...
    def fetch_all(self): ...


class EmailSender:
    """Handles email composition and sending."""
    def __init__(self, config: EmailConfig):
        self.config = config
        self.formatter = EmailFormatter()
    
    def send_digest(self, items: List[DigestItem]): ...
    def add_recipient(self, email: str): ...


class ContentProcessor:
    """Processes and enhances content."""
    def __init__(self, openai_client):
        self.openai_client = openai_client
        self.summarizer = ContentSummarizer(openai_client)
        self.ranker = ContentRanker()
    
    def process_items(self, items: List[Dict]) -> List[DigestItem]: ...


class DigestScheduler:
    """Manages digest scheduling."""
    def __init__(self, schedule_config: ScheduleConfig):
        self.config = schedule_config
        self.last_run = None
    
    def should_run_now(self) -> bool: ...
    def update_last_run(self): ...


# Orchestrator class that uses the others
class EmailDigestService:
    """Orchestrates the email digest workflow."""
    def __init__(self):
        self.feed_manager = FeedManager()
        self.email_sender = EmailSender(EmailConfig())
        self.content_processor = ContentProcessor(OpenAI())
        self.scheduler = DigestScheduler(ScheduleConfig())
    
    def run_digest(self):
        if not self.scheduler.should_run_now():
            return
            
        items = self.feed_manager.fetch_all()
        processed_items = self.content_processor.process_items(items)
        self.email_sender.send_digest(processed_items)
        self.scheduler.update_last_run()
```

**Potential Problems:**
* Low cohesion and high coupling.
* Difficult to understand, test, and maintain.
* Often leads to `smell_duplicate_code`.
* Hard to reuse in other contexts.

**Refactorings (Solutions):**
* **Extract Class:** (See Rule: `ref_extract_class`)
* **Extract Subclass:** (See Rule: `ref_extract_subclass`)
* **Extract Interface:** (See Rule: `ref_extract_interface`)
* **Replace Data Value with Object:** (See Rule: `ref_replace_data_value_with_object`)
* **Replace Type Code with Subclasses:** (See Rule: `ref_replace_type_code_with_subclasses`)

**Tags:** `#bloaters`, `#organization`, `#single-responsibility`

---

## Smell: Primitive Obsession

**ID:** `smell_primitive_obsession`
**Severity:** Medium
**Description:** Over-reliance on primitive data types instead of creating small, meaningful objects for simple tasks (e.g., using strings for phone numbers, currency, or special codes).

**Detection Patterns:**
- **Patterns:**
  - String validation with regex in multiple places
  - Dictionaries used as pseudo-objects
  - Type codes as strings/integers
  - Lists of primitives that always appear together
- **AST Pattern:** Multiple string operations on the same variable pattern
- **Tools:**
  ```bash
  # grep for common patterns:
  grep -r "if.*==.*['\"]" .  # String comparisons
  grep -r "split\|strip\|replace" . # String manipulation
  ```

**Code Example (Before):**
```python
def process_email_config(from_email: str, to_emails: str, smtp_host: str, smtp_port: int):
    """Process email configuration."""
    # Validate from_email
    if not re.match(r'^[\w\.-]+@[\w\.-]+\.\w+$', from_email):
        raise ValueError("Invalid from email")
    
    # Parse to_emails (comma-separated string)
    recipient_list = []
    for email in to_emails.split(','):
        email = email.strip()
        if re.match(r'^[\w\.-]+@[\w\.-]+\.\w+$', email):
            recipient_list.append(email)
    
    # Validate SMTP settings
    if smtp_port not in [25, 465, 587, 2525]:
        raise ValueError("Invalid SMTP port")
    
    # Create connection string
    connection_string = f"{smtp_host}:{smtp_port}"
    
    return {
        'from': from_email,
        'to': recipient_list,
        'connection': connection_string
    }
```

**Code Example (After):**
```python
from dataclasses import dataclass
from typing import List
import re

class Email:
    """Represents a validated email address."""
    def __init__(self, address: str):
        if not self._is_valid(address):
            raise ValueError(f"Invalid email address: {address}")
        self.address = address.strip().lower()
    
    @staticmethod
    def _is_valid(address: str) -> bool:
        return bool(re.match(r'^[\w\.-]+@[\w\.-]+\.\w+$', address))
    
    def __str__(self):
        return self.address
    
    @property
    def domain(self) -> str:
        return self.address.split('@')[1]


class SmtpPort:
    """Represents a valid SMTP port."""
    VALID_PORTS = {25, 465, 587, 2525}
    
    def __init__(self, port: int):
        if port not in self.VALID_PORTS:
            raise ValueError(f"Invalid SMTP port: {port}. Must be one of {self.VALID_PORTS}")
        self.value = port
    
    @property
    def is_secure(self) -> bool:
        return self.value in {465, 587}


@dataclass
class SmtpConnection:
    """Represents SMTP connection details."""
    host: str
    port: SmtpPort
    
    @property
    def connection_string(self) -> str:
        return f"{self.host}:{self.port.value}"
    
    @property
    def requires_tls(self) -> bool:
        return self.port.is_secure


@dataclass
class EmailConfig:
    """Email configuration with validated components."""
    sender: Email
    recipients: List[Email]
    smtp: SmtpConnection
    
    @classmethod
    def from_strings(cls, from_email: str, to_emails: str, smtp_host: str, smtp_port: int):
        sender = Email(from_email)
        recipients = [Email(addr) for addr in to_emails.split(',') if addr.strip()]
        smtp = SmtpConnection(smtp_host, SmtpPort(smtp_port))
        return cls(sender, recipients, smtp)
```

**Potential Problems:**
* Loss of type safety and semantic meaning.
* Related data and behavior are scattered.
* Can lead to `smell_data_clumps` or long parameter lists.
* Implicit assumptions about primitive values (e.g., format, range).

**Refactorings (Solutions):**
* **Replace Data Value with Object:** (See Rule: `ref_replace_data_value_with_object`)
* **Replace Type Code with Class/Subclasses/State/Strategy:** (See Rule: `ref_replace_type_code_with_subclasses`, `ref_replace_type_code_with_state_strategy`)
* **Introduce Parameter Object:** (See Rule: `ref_introduce_parameter_object`)
* **Extract Class:** (See Rule: `ref_extract_class`) (If multiple primitives represent a coherent concept)

**Tags:** `#bloaters`, `#dispensables`, `#encapsulation`

---

## NEW SMELLS - Project Specific

---

## Smell: Missing Error Handling

**ID:** `smell_missing_error_handling`
**Severity:** High
**Description:** Code that can fail but lacks proper error handling, leading to crashes or silent failures.

**Detection Patterns:**
- **Patterns:**
  - API calls without try/except
  - File operations without error handling
  - Network requests without timeout/retry
  - Missing None checks after .get()
- **AST Pattern:** Function calls known to raise exceptions without surrounding try/except
- **Tools:**
  ```bash
  # Find potential issues
  grep -r "requests\." . | grep -v "try:"
  grep -r "open(" . | grep -v "with"
  grep -r "\.get(" . | grep -v "if.*is not None"
  ```

**Code Example (Before):**
```python
def fetch_and_summarize(url: str) -> str:
    """Fetch content and summarize it."""
    # Missing error handling for network request
    response = requests.get(url)
    content = response.text
    
    # Missing error handling for API call
    summary = openai_client.summarize(content)
    
    # Missing None check
    metadata = response.headers.get('last-modified')
    formatted_date = datetime.strptime(metadata, '%a, %d %b %Y %H:%M:%S GMT')
    
    return summary
```

**Code Example (After):**
```python
def fetch_and_summarize(url: str) -> Optional[str]:
    """Fetch content and summarize it with proper error handling."""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()
    except requests.RequestException as e:
        logger.error(f"Failed to fetch {url}: {e}")
        return None
    
    try:
        summary = openai_client.summarize(response.text)
    except OpenAIError as e:
        logger.error(f"Failed to summarize content: {e}")
        # Fallback to truncated content
        return response.text[:200] + "..."
    
    # Safe metadata handling
    metadata = response.headers.get('last-modified')
    if metadata:
        try:
            formatted_date = datetime.strptime(metadata, '%a, %d %b %Y %H:%M:%S GMT')
            logger.info(f"Content last modified: {formatted_date}")
        except ValueError:
            logger.warning(f"Invalid date format: {metadata}")
    
    return summary
```

**Refactorings (Solutions):**
* Add try/except blocks
* Use context managers (with statements)
* Add retry logic with backoff
* Implement circuit breakers for external services
* Add None checks and default values

**Tags:** `#robustness`, `#error-handling`, `#defensive-programming`

---

## Smell: Hard-coded Configuration

**ID:** `smell_hardcoded_configuration`
**Severity:** Medium
**Description:** Configuration values embedded directly in code instead of being externalized.

**Detection Patterns:**
- **Patterns:**
  - URLs, API endpoints in code
  - Timeout values, retry counts
  - File paths (especially absolute paths)
  - API keys or credentials (CRITICAL)
- **Tools:**
  ```bash
  # Find potential hardcoded values
  grep -r "http://" . --include="*.py"
  grep -r "https://" . --include="*.py" 
  grep -r "[0-9]\{2,\}" . --include="*.py" # Multi-digit numbers
  grep -r '"/.*/"' . --include="*.py" # Absolute paths
  ```

**Code Example (Before):**
```python
class FeedFetcher:
    def __init__(self):
        self.feeds = [
            "https://aws.amazon.com/blogs/aws/feed/",
            "https://openai.com/blog/rss.xml",
            "https://blog.github.com/feed.xml"
        ]
        self.timeout = 30
        self.max_retries = 3
        self.cache_dir = "/tmp/feed_cache"
    
    def fetch_feeds(self):
        for feed_url in self.feeds:
            for attempt in range(self.max_retries):
                try:
                    response = requests.get(feed_url, timeout=self.timeout)
                    # Process response
                except:
                    if attempt == self.max_retries - 1:
                        raise
```

**Code Example (After):**
```python
from dataclasses import dataclass
from pathlib import Path
import os
from typing import List

@dataclass
class FeedConfig:
    """Feed fetcher configuration."""
    urls: List[str]
    timeout: int = 30
    max_retries: int = 3
    cache_dir: Path = Path("/tmp/feed_cache")
    
    @classmethod
    def from_env(cls):
        """Load configuration from environment variables."""
        return cls(
            urls=os.getenv('FEED_URLS', '').split(','),
            timeout=int(os.getenv('FEED_TIMEOUT', '30')),
            max_retries=int(os.getenv('FEED_MAX_RETRIES', '3')),
            cache_dir=Path(os.getenv('FEED_CACHE_DIR', '/tmp/feed_cache'))
        )
    
    @classmethod
    def from_file(cls, config_path: str):
        """Load configuration from YAML/JSON file."""
        import yaml
        with open(config_path) as f:
            data = yaml.safe_load(f)
        return cls(**data['feed_fetcher'])


class FeedFetcher:
    def __init__(self, config: FeedConfig):
        self.config = config
        self.config.cache_dir.mkdir(exist_ok=True)
    
    def fetch_feeds(self):
        for feed_url in self.config.urls:
            self._fetch_with_retry(feed_url)
    
    def _fetch_with_retry(self, url: str):
        for attempt in range(self.config.max_retries):
            try:
                response = requests.get(url, timeout=self.config.timeout)
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                if attempt == self.config.max_retries - 1:
                    raise
                logger.warning(f"Attempt {attempt + 1} failed for {url}: {e}")
```

**Refactorings (Solutions):**
* Extract to configuration files (YAML, JSON, TOML)
* Use environment variables
* Create configuration classes
* Use dependency injection
* Implement configuration validation

**Tags:** `#configuration`, `#maintainability`, `#deployment`

---

## Smell: Inconsistent Naming

**ID:** `smell_inconsistent_naming`
**Severity:** Low
**Description:** Mixed naming conventions within the same codebase or module.

**Detection Patterns:**
- **Patterns:**
  - camelCase vs snake_case mixing
  - Inconsistent abbreviations
  - Unclear variable names (x, data, temp)
  - Different naming for similar concepts
- **Tools:**
  ```bash
  # Find potential naming issues
  grep -r "[a-z][A-Z]" . --include="*.py" # camelCase
  grep -r "\b[a-z]\b" . --include="*.py" # Single letter vars
  # Use pylint naming conventions
  ```

**Code Example (Before):**
```python
class email_processor:  # Should be EmailProcessor
    def __init__(self):
        self.emailList = []  # Should be email_list
        self.MAX_ITEMS = 100  # Should be defined as class constant
        self.tmp = {}  # Unclear name
        
    def ProcessEmail(self, emailData):  # Should be process_email
        e = emailData.get('email')  # Single letter variable
        if e:
            self.emailList.append(e)
            
    def get_emails_count(self):  # Inconsistent with other method names
        return len(self.emailList)
```

**Code Example (After):**
```python
class EmailProcessor:
    """Processes email data according to business rules."""
    
    MAX_ITEMS = 100  # Class constant
    
    def __init__(self):
        self.email_list: List[str] = []
        self.processing_cache: Dict[str, Any] = {}
        
    def process_email(self, email_data: Dict[str, Any]) -> None:
        """Process a single email entry."""
        email_address = email_data.get('email')
        if email_address and self._is_valid_email(email_address):
            self.email_list.append(email_address)
            
    def get_email_count(self) -> int:
        """Return the count of processed emails."""
        return len(self.email_list)
    
    def _is_valid_email(self, email_address: str) -> bool:
        """Validate email address format."""
        return '@' in email_address  # Simplified for example
```

**Refactorings (Solutions):**
* Rename variables/functions/classes
* Establish naming conventions document
* Use linting tools with naming rules
* Regular code reviews focusing on naming

**Tags:** `#readability`, `#consistency`, `#maintainability`

---

## Smell: Global State Abuse

**ID:** `smell_global_state_abuse`
**Severity:** High
**Description:** Excessive use of global variables or module-level mutable state that can be modified from anywhere.

**Detection Patterns:**
- **Patterns:**
  - Global variables (uppercase at module level)
  - Singleton pattern abuse
  - Module-level mutable collections
  - Hidden dependencies on global state
- **Tools:**
  ```bash
  # Find global variables
  grep -r "^[A-Z_]*\s*=" . --include="*.py" | grep -v "class\|def"
  grep -r "global " . --include="*.py"
  ```

**Code Example (Before):**
```python
# config.py
DATABASE_CONFIG = {}
ACTIVE_CONNECTIONS = []
CACHE = {}
REQUEST_COUNT = 0

# service.py
from config import DATABASE_CONFIG, ACTIVE_CONNECTIONS, CACHE, REQUEST_COUNT

def connect_to_database():
    global ACTIVE_CONNECTIONS, REQUEST_COUNT
    REQUEST_COUNT += 1
    
    conn = create_connection(DATABASE_CONFIG)
    ACTIVE_CONNECTIONS.append(conn)
    return conn

def get_cached_data(key):
    global CACHE, REQUEST_COUNT
    REQUEST_COUNT += 1
    
    if key in CACHE:
        return CACHE[key]
    
    data = fetch_data(key)
    CACHE[key] = data
    return data

def cleanup():
    global ACTIVE_CONNECTIONS, CACHE
    for conn in ACTIVE_CONNECTIONS:
        conn.close()
    ACTIVE_CONNECTIONS.clear()
    CACHE.clear()
```

**Code Example (After):**
```python
# config.py
from dataclasses import dataclass, field
from typing import Dict, List, Any
import threading

@dataclass
class DatabaseConfig:
    host: str
    port: int
    database: str
    
@dataclass
class ApplicationState:
    """Encapsulates application state with thread-safe access."""
    database_config: DatabaseConfig
    _active_connections: List[Any] = field(default_factory=list)
    _cache: Dict[str, Any] = field(default_factory=dict)
    _request_count: int = 0
    _lock: threading.Lock = field(default_factory=threading.Lock)
    
    def increment_request_count(self):
        with self._lock:
            self._request_count += 1
            
    def add_connection(self, conn):
        with self._lock:
            self._active_connections.append(conn)
            
    def get_connections(self):
        with self._lock:
            return self._active_connections.copy()
            
    def cache_get(self, key: str):
        with self._lock:
            return self._cache.get(key)
            
    def cache_set(self, key: str, value: Any):
        with self._lock:
            self._cache[key] = value

# service.py
class DatabaseService:
    def __init__(self, app_state: ApplicationState):
        self.app_state = app_state
        
    def connect(self):
        self.app_state.increment_request_count()
        conn = create_connection(self.app_state.database_config)
        self.app_state.add_connection(conn)
        return conn
        
    def cleanup(self):
        for conn in self.app_state.get_connections():
            conn.close()

class CacheService:
    def __init__(self, app_state: ApplicationState):
        self.app_state = app_state
        
    def get_data(self, key: str):
        self.app_state.increment_request_count()
        
        cached = self.app_state.cache_get(key)
        if cached is not None:
            return cached
            
        data = self._fetch_data(key)
        self.app_state.cache_set(key, data)
        return data
        
    def _fetch_data(self, key: str):
        # Fetch implementation
        pass
```

**Refactorings (Solutions):**
* Encapsulate global state in classes
* Use dependency injection
* Pass state explicitly as parameters
* Use context managers for state management
* Implement proper state containers

**Tags:** `#coupling`, `#testability`, `#thread-safety`

---

## Smell: Untested Complex Logic

**ID:** `smell_untested_complex_logic`
**Severity:** High
**Description:** Complex business logic or algorithms without corresponding unit tests.

**Detection Patterns:**
- **Patterns:**
  - High cyclomatic complexity without tests
  - Financial calculations without tests
  - Data transformation logic without tests
  - External API integration without mocks
- **Tools:**
  ```bash
  # Find complex functions
  radon cc -nc . | grep -E "C |D |E |F "
  # Check test coverage
  pytest --cov=src --cov-report=term-missing
  ```

**Code Example (Before):**
```python
# digest_scorer.py - No tests!
def score_digest_items(items: List[Dict]) -> List[Dict]:
    """Score and rank digest items based on relevance."""
    scored_items = []
    
    for item in items:
        score = 0
        
        # Recency scoring
        pub_date = datetime.fromisoformat(item['published'])
        days_old = (datetime.now() - pub_date).days
        if days_old == 0:
            score += 10
        elif days_old == 1:
            score += 7
        elif days_old <= 3:
            score += 4
        elif days_old <= 7:
            score += 2
            
        # Keyword scoring
        title_lower = item['title'].lower()
        high_priority_keywords = ['breaking', 'announced', 'launched', 'critical']
        medium_priority_keywords = ['update', 'new', 'improved', 'beta']
        
        for keyword in high_priority_keywords:
            if keyword in title_lower:
                score += 5
                
        for keyword in medium_priority_keywords:
            if keyword in title_lower:
                score += 2
                
        # Source scoring
        trusted_sources = ['aws.amazon.com', 'openai.com', 'github.blog']
        if any(source in item.get('link', '') for source in trusted_sources):
            score += 3
            
        # Length scoring (prefer medium-length summaries)
        summary_length = len(item.get('summary', ''))
        if 100 <= summary_length <= 300:
            score += 2
        elif summary_length > 300:
            score -= 1
            
        item['relevance_score'] = score
        scored_items.append(item)
    
    # Sort by score descending
    return sorted(scored_items, key=lambda x: x['relevance_score'], reverse=True)
```

**Code Example (After - with tests):**
```python
# digest_scorer.py
from datetime import datetime, timedelta
from typing import List, Dict
from dataclasses import dataclass

@dataclass
class ScoringConfig:
    """Configuration for digest item scoring."""
    recency_scores = {0: 10, 1: 7, 3: 4, 7: 2}  # days: score
    high_priority_keywords = ['breaking', 'announced', 'launched', 'critical']
    medium_priority_keywords = ['update', 'new', 'improved', 'beta']
    trusted_sources = ['aws.amazon.com', 'openai.com', 'github.blog']
    keyword_scores = {'high': 5, 'medium': 2}
    source_trust_score = 3
    optimal_summary_length = (100, 300)
    

class DigestScorer:
    """Scores digest items based on various relevance factors."""
    
    def __init__(self, config: ScoringConfig = None):
        self.config = config or ScoringConfig()
        
    def score_items(self, items: List[Dict]) -> List[Dict]:
        """Score and rank digest items."""
        scored_items = [self._score_item(item.copy()) for item in items]
        return sorted(scored_items, key=lambda x: x['relevance_score'], reverse=True)
    
    def _score_item(self, item: Dict) -> Dict:
        """Calculate relevance score for a single item."""
        score = 0
        score += self._calculate_recency_score(item.get('published'))
        score += self._calculate_keyword_score(item.get('title', ''))
        score += self._calculate_source_score(item.get('link', ''))
        score += self._calculate_length_score(item.get('summary', ''))
        
        item['relevance_score'] = score
        item['score_breakdown'] = {
            'recency': self._calculate_recency_score(item.get('published')),
            'keywords': self._calculate_keyword_score(item.get('title', '')),
            'source': self._calculate_source_score(item.get('link', '')),
            'length': self._calculate_length_score(item.get('summary', ''))
        }
        return item
    
    def _calculate_recency_score(self, published: str) -> int:
        """Score based on publication recency."""
        if not published:
            return 0
            
        try:
            pub_date = datetime.fromisoformat(published)
            days_old = (datetime.now() - pub_date).days
            
            for max_days, score in sorted(self.config.recency_scores.items()):
                if days_old <= max_days:
                    return score
            return 0
        except ValueError:
            return 0
    
    def _calculate_keyword_score(self, title: str) -> int:
        """Score based on keyword presence."""
        if not title:
            return 0
            
        score = 0
        title_lower = title.lower()
        
        for keyword in self.config.high_priority_keywords:
            if keyword in title_lower:
                score += self.config.keyword_scores['high']
                
        for keyword in self.config.medium_priority_keywords:
            if keyword in title_lower:
                score += self.config.keyword_scores['medium']
                
        return score
    
    def _calculate_source_score(self, link: str) -> int:
        """Score based on source trustworthiness."""
        if not link:
            return 0
            
        for source in self.config.trusted_sources:
            if source in link:
                return self.config.source_trust_score
        return 0
    
    def _calculate_length_score(self, summary: str) -> int:
        """Score based on summary length."""
        if not summary:
            return -1
            
        length = len(summary)
        min_len, max_len = self.config.optimal_summary_length
        
        if min_len <= length <= max_len:
            return 2
        elif length > max_len:
            return -1
        return 0


# test_digest_scorer.py
import pytest
from datetime import datetime, timedelta
from digest_scorer import DigestScorer, ScoringConfig

class TestDigestScorer:
    
    @pytest.fixture
    def scorer(self):
        return DigestScorer()
    
    @pytest.fixture
    def sample_item(self):
        return {
            'title': 'AWS Announced New Feature',
            'link': 'https://aws.amazon.com/blogs/feature',
            'published': datetime.now().isoformat(),
            'summary': 'A' * 200  # 200 chars
        }
    
    def test_recency_scoring(self, scorer):
        """Test that recency scores work correctly."""
        now = datetime.now()
        test_cases = [
            (now, 10),  # Today
            (now - timedelta(days=1), 7),  # Yesterday
            (now - timedelta(days=2), 4),  # 2 days ago
            (now - timedelta(days=5), 2),  # 5 days ago
            (now - timedelta(days=10), 0),  # Too old
        ]
        
        for date, expected_score in test_cases:
            score = scorer._calculate_recency_score(date.isoformat())
            assert score == expected_score, f"Date {date} should score {expected_score}, got {score}"
    
    def test_keyword_scoring(self, scorer):
        """Test keyword detection and scoring."""
        test_cases = [
            ("Regular title", 0),
            ("Breaking: Major announcement", 5),  # High priority
            ("New update available", 2),  # Medium priority
            ("Critical security update launched", 7),  # Both high and medium
        ]
        
        for title, expected_score in test_cases:
            score = scorer._calculate_keyword_score(title)
            assert score == expected_score, f"Title '{title}' should score {expected_score}, got {score}"
    
    def test_source_scoring(self, scorer):
        """Test source trust scoring."""
        test_cases = [
            ("https://aws.amazon.com/blog/post", 3),
            ("https://random-blog.com/post", 0),
            ("https://openai.com/research", 3),
        ]
        
        for link, expected_score in test_cases:
            score = scorer._calculate_source_score(link)
            assert score == expected_score
    
    def test_length_scoring(self, scorer):
        """Test summary length scoring."""
        test_cases = [
            ("A" * 50, -1),   # Too short
            ("A" * 150, 2),   # Optimal
            ("A" * 250, 2),   # Optimal
            ("A" * 400, -1),  # Too long
            ("", -1),         # Empty
        ]
        
        for summary, expected_score in test_cases:
            score = scorer._calculate_length_score(summary)
            assert score == expected_score
    
    def test_full_scoring(self, scorer, sample_item):
        """Test complete item scoring."""
        scored_item = scorer._score_item(sample_item.copy())
        
        # Check total score calculation
        expected_score = 10 + 5 + 3 + 2  # recency + keyword + source + length
        assert scored_item['relevance_score'] == expected_score
        
        # Check score breakdown
        assert 'score_breakdown' in scored_item
        assert scored_item['score_breakdown']['recency'] == 10
        assert scored_item['score_breakdown']['keywords'] == 5
    
    def test_sorting(self, scorer):
        """Test that items are sorted by score correctly."""
        items = [
            {'title': 'Old news', 'published': (datetime.now() - timedelta(days=10)).isoformat()},
            {'title': 'Breaking news', 'published': datetime.now().isoformat()},
            {'title': 'Yesterday update', 'published': (datetime.now() - timedelta(days=1)).isoformat()},
        ]
        
        sorted_items = scorer.score_items(items)
        scores = [item['relevance_score'] for item in sorted_items]
        assert scores == sorted(scores, reverse=True)
```

**Refactorings (Solutions):**
* Write unit tests for all complex logic
* Use test-driven development (TDD)
* Mock external dependencies
* Aim for high test coverage (>80%)
* Test edge cases and error conditions

**Tags:** `#testing`, `#quality`, `#maintainability`

---

## Smell: Async/Await Confusion

**ID:** `smell_async_await_confusion`
**Severity:** Medium
**Description:** Incorrect or inconsistent use of async/await patterns, leading to performance issues or bugs.

**Detection Patterns:**
- **Patterns:**
  - Mixing sync and async inappropriately
  - Missing await statements
  - Unnecessary async functions
  - Blocking operations in async functions
- **Tools:**
  ```bash
  # Find potential async issues
  grep -r "async def" . --include="*.py" | grep -v "await"
  grep -r "asyncio.run" . --include="*.py"
  ```

**Code Example (Before):**
```python
import asyncio
import requests  # Blocking library!

async def fetch_feeds(urls):
    """Incorrectly implemented async function."""
    results = []
    
    # Bad: Using blocking requests in async function
    for url in urls:
        response = requests.get(url)  # This blocks!
        results.append(response.text)
    
    return results

async def process_items(items):
    """Unnecessary async function."""
    # No await statements, doesn't need to be async
    processed = []
    for item in items:
        processed.append(item.upper())
    return processed

def mixed_sync_async():
    """Mixing sync and async incorrectly."""
    # Bad: Creating event loop in sync function
    loop = asyncio.get_event_loop()
    urls = ["url1", "url2"]
    
    # Bad: Not handling async properly
    task = fetch_feeds(urls)  # Returns coroutine, not result!
    print(task)  # Prints coroutine object
```

**Code Example (After):**
```python
import asyncio
import aiohttp
from typing import List

async def fetch_feeds(urls: List[str]) -> List[str]:
    """Properly implemented async function."""
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_single_feed(session, url) for url in urls]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        valid_results = []
        for url, result in zip(urls, results):
            if isinstance(result, Exception):
                logger.error(f"Failed to fetch {url}: {result}")
            else:
                valid_results.append(result)
        
        return valid_results

async def fetch_single_feed(session: aiohttp.ClientSession, url: str) -> str:
    """Fetch a single feed with timeout and error handling."""
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=30)) as response:
            response.raise_for_status()
            return await response.text()
    except asyncio.TimeoutError:
        raise Exception(f"Timeout fetching {url}")
    except aiohttp.ClientError as e:
        raise Exception(f"Error fetching {url}: {e}")

def process_items(items: List[str]) -> List[str]:
    """Correctly implemented as sync function."""
    # No async operations, so no need for async
    return [item.upper() for item in items]

async def process_items_async(items: List[str]) -> List[str]:
    """Async version if CPU-intensive processing is needed."""
    # Use thread pool for CPU-bound operations
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(None, process_items, items)

async def main():
    """Proper async entry point."""
    urls = ["https://example.com/feed1", "https://example.com/feed2"]
    
    # Fetch feeds concurrently
    feeds = await fetch_feeds(urls)
    
    # Process if needed
    if feeds:
        processed = await process_items_async(feeds)
        return processed
    
    return []

# Proper way to run async code
if __name__ == "__main__":
    result = asyncio.run(main())
    print(f"Processed {len(result)} feeds")
```

**Refactorings (Solutions):**
* Use aiohttp instead of requests for async HTTP
* Properly await all async operations
* Don't make functions async unless needed
* Use asyncio.gather for concurrent operations
* Handle exceptions in async code properly

**Tags:** `#async`, `#performance`, `#concurrency`

---

## Summary of Detection Patterns

To automatically detect these code smells, you can create a configuration file for your linting tools:

```yaml
# .smell-detection.yml
smells:
  long_method:
    max_lines: 20
    max_complexity: 10
    
  large_class:
    max_lines: 250
    max_methods: 20
    max_attributes: 10
    
  primitive_obsession:
    patterns:
      - regex: 'if.*==.*["\']'
      - regex: '\.split\(.*\)\.strip\(\)'
      
  missing_error_handling:
    dangerous_calls:
      - requests.get
      - requests.post
      - open
      - json.loads
      - datetime.strptime
      
  hardcoded_configuration:
    patterns:
      - regex: 'https?://'
      - regex: ':[0-9]{2,5}'  # Port numbers
      - regex: '^[A-Z_]+\s*=\s*["\']'  # String constants
```

## Project-Specific Code Smells

### Smell: Unhandled API Rate Limits
**ID:** `smell_unhandled_rate_limits`
**Severity:** High
**Description:** API calls without rate limit handling, risking service disruption.

**Detection Pattern:**
```bash
# Find API calls without rate limit handling
grep -r "openai\." . --include="*.py" | grep -v "retry\|RateLimitError"
grep -r "sendgrid\." . --include="*.py" | grep -v "retry\|RateLimitError"
```

**Refactoring:** Implement retry decorators with exponential backoff for all external API calls.

### Smell: Missing Feed Validation
**ID:** `smell_missing_feed_validation`
**Severity:** Medium
**Description:** RSS feed parsing without proper validation and sanitization.

**Refactoring:** Create FeedValidator class with entry validation, URL sanitization, and HTML cleaning.

## Tool Configuration

### Flake8 Configuration (.flake8)
```ini
[flake8]
max-line-length = 88
max-complexity = 10
ignore = E203, E501, W503
exclude = .git,__pycache__,docs/source/conf.py,old,build,dist,.venv
per-file-ignores = __init__.py:F401
```

### Pylint Configuration (.pylintrc)
```ini
[MASTER]
extension-pkg-whitelist=pydantic

[MESSAGES CONTROL]
disable=missing-docstring,too-few-public-methods,import-error

[FORMAT]
max-line-length=88
```

### Pre-commit Hooks (.pre-commit-config.yaml)
```yaml
repos:
  - repo: https://github.com/psf/black
    rev: 22.3.0
    hooks:
      - id: black
  - repo: https://github.com/pycqa/flake8
    rev: 4.0.1
    hooks:
      - id: flake8
  - repo: https://github.com/pycqa/isort
    rev: 5.10.1
    hooks:
      - id: isort
```

## Refactoring Prioritization Framework

### Priority Matrix
- **Critical (P0)**: Security vulnerabilities, API rate limit violations
- **High (P1)**: Performance issues, error handling gaps
- **Medium (P2)**: Code clarity, maintainability improvements
- **Low (P3)**: Style consistency, documentation

### Implementation Strategy
1. **Incremental Refactoring**: Small, focused changes over time
2. **Test-Driven Refactoring**: Write tests before refactoring
3. **Backward Compatibility**: Maintain API compatibility during refactoring
4. **Monitoring**: Track performance impact of refactoring changes

---

This enhanced version provides concrete examples, detection patterns, project-specific guidance, and automated tooling integration for the Vibe Coding Digest project.