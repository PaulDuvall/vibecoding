# ATDD/BDD Rules for AI-Driven Development

This file provides comprehensive guidelines for implementing **Acceptance Test-Driven Development (ATDD)** and **Behavior-Driven Development (BDD)** based on [Paul Duvall's ATDD-driven AI development methodology](https://www.paulmduvall.com/atdd-driven-ai-development-how-prompting-and-tests-steer-the-code/).

## Core Philosophy

> **"Prompting is the new coding: In agentic development, my main job is to write clear, structured, and precise prompts/tests"**

ATDD places executable specifications at the center of development, with AI as an implementation partner that transforms behavioral requirements into working code.

---

## 1. Specification-First Development

### Rule 1.1: Write Executable Specifications Before Implementation
- **Always** start with Gherkin feature files that describe desired behavior
- Define system behavior from the **user's perspective**, not technical implementation
- Use **Given-When-Then** structure for clear scenario definition

**Good Example:**
```gherkin
Feature: US-001 - Daily Digest Generation
  As a content consumer interested in AI and developer tools
  I want to receive a daily email digest with curated, summarized content
  So that I can stay updated on the latest developments efficiently

  Scenario: US-001 US-003 - Complete daily digest workflow execution
    Given the system has valid API keys for OpenAI and SendGrid
    And the recipient email is configured
    And RSS feeds are accessible
    When the daily digest generation process is executed
    Then content should be fetched from all configured RSS feeds
    And articles should be summarized using OpenAI with Paul Duvall's voice
    And a properly formatted HTML email should be generated
    And the email should be sent successfully via SendGrid
```

**Bad Example:**
```gherkin
# Too technical, implementation-focused
Scenario: Test database connection
  Given database is running on port 5432
  When SQL query is executed
  Then return status code 200
```

### Rule 1.2: Link Features to User Stories
- Use **US-XXX** naming pattern in scenario titles
- Maintain traceability between user stories and feature files
- Support multiple user story mapping (e.g., `US-001 US-003`)

---

## 2. Test Architecture and Organization

### Rule 2.1: Directory Structure
Organize ATDD tests with clear separation of concerns:

```
project/
├── tests/features/           # Gherkin specifications
│   ├── digest_workflow.feature
│   ├── environment.py        # Behave configuration
│   └── steps/               # Step definitions
│       ├── __init__.py
│       └── digest_steps.py
├── .behaverc               # Behave configuration file
└── src/                    # Implementation code
```

### Rule 2.2: Feature File Naming
- Use descriptive names: `digest_workflow.feature`, `user_authentication.feature`
- Group related scenarios in single feature files
- Keep features focused on specific business capabilities

### Rule 2.3: Step Definition Organization
- **One step file per feature domain** (e.g., `digest_steps.py`, `auth_steps.py`)
- Use clear, descriptive function names
- Group related steps logically within files

---

## 3. Real Implementation Testing (Not Pure Mocking)

### Rule 3.1: Test Real Business Logic
ATDD tests must call **actual implementation code**, not just verify mocks.

**Good Example:**
```python
@when('the daily digest generation process is executed')
def step_execute_digest_generation(context):
    """Execute the digest generation process with mocks for external services only."""
    with patch('feedparser.parse') as mock_feedparser, \
         patch('openai.OpenAI') as mock_openai_client, \
         patch('requests.post') as mock_sendgrid:
        
        # Call the REAL implementation functions
        from src.vibe_digest import gather_feed_items, dedupe_and_sort_items, summarize_items, format_digest
        
        all_items = gather_feed_items()  # Real feed processing logic
        unique_items = dedupe_and_sort_items(all_items)  # Real deduplication logic
        summaries = summarize_items(unique_items)  # Real summarization logic
        html, md = format_digest(summaries)  # Real formatting logic
```

**Bad Example:**
```python
@when('the daily digest generation process is executed')
def step_execute_digest_generation(context):
    # Pure mock - doesn't test real implementation
    context.result = Mock()
    context.result.success = True
```

### Rule 3.2: Mock External Dependencies Only
- **Mock external services**: APIs, databases, network calls, file systems
- **Test real business logic**: algorithms, data transformations, validation rules
- **Use realistic mock data** that mirrors production scenarios

### Rule 3.3: Cross-Platform Compatibility
- Ensure tests work identically in **local development** and **CI/CD environments**
- Use compatible mock objects (avoid Mock comparison issues)
- Set consistent environment variables across platforms

---

## 4. AI-Driven Implementation Workflow

### Rule 4.1: RED-GREEN-REFACTOR with AI
Follow strict TDD cycle with AI as implementation partner:

1. **RED**: Write failing ATDD test (Gherkin scenario + step definitions)
2. **GREEN**: Prompt AI to implement minimal code to pass the test
3. **REFACTOR**: Use AI to improve code quality while maintaining test coverage

### Rule 4.2: Iterative Feature Development
- Implement **one scenario at a time**
- Verify each scenario passes before moving to the next
- Run full test suite after each implementation to ensure no regressions

### Rule 4.3: AI Prompting Best Practices
When prompting AI for implementation:
- **Provide context**: Show failing tests and desired behavior
- **Be specific**: Include acceptance criteria and edge cases
- **Request explanations**: Ask AI to explain implementation decisions
- **Iterate**: Refine prompts based on AI output quality

---

## 5. Traceability and Documentation

### Rule 5.1: Maintain Comprehensive Traceability Matrix
Document relationships between:
- User stories (US-XXX)
- Feature files
- Step definitions
- Implementation code
- Unit/integration tests

**Example:**
| User Story ID | ATDD Scenario(s) | Implementation File(s) | Test File(s) |
|--------------|------------------|------------------------|--------------|
| US-001 | Complete daily digest workflow execution | src/vibe_digest.py, src/feeds.py | tests/test_vibe_digest.py |
| US-002 | OpenAI API rate limiting scenario | src/summarize.py | tests/test_optimizations.py |

### Rule 5.2: Living Documentation
- Keep feature files as **living documentation** that stays current with implementation
- Update scenarios when business requirements change
- Use feature files as primary specification source for stakeholders

---

## 6. Quality and Coverage Standards

### Rule 6.1: Comprehensive Scenario Coverage
- Cover **happy path** scenarios (primary user flows)
- Include **error handling** scenarios (network failures, API errors)
- Test **edge cases** (empty data, rate limits, timeouts)
- Validate **integration points** between components

### Rule 6.2: Realistic Test Data
- Use **production-like data** in scenarios
- Include actual URLs, realistic timestamps, proper data formats
- Avoid trivial test data that doesn't reflect real usage

### Rule 6.3: Performance and Non-Functional Requirements
Include scenarios for:
- **Performance requirements**: "process should complete within 5 minutes"
- **Scalability**: "system should handle 25+ RSS feed sources"
- **Reliability**: "failed feeds should not stop the process"
- **Security**: "API keys should be validated"

---

## 7. CI/CD Integration

### Rule 7.1: Automated ATDD Execution
- Run ATDD tests in **CI/CD pipeline**
- Set up proper environment variables for test execution
- Use consistent test execution commands across environments

### Rule 7.2: Test Environment Parity
- Ensure **identical behavior** between local and CI/CD environments
- Use same test data, environment variables, and execution parameters
- Validate test results are reproducible across platforms

### Rule 7.3: Failure Handling
- ATDD test failures should **block deployment**
- Provide clear failure messages that link to specific user stories
- Include logs and context for debugging failed scenarios

---

## 8. Anti-Patterns to Avoid

### Anti-Pattern 8.1: Pure Mock Testing
❌ **Don't** write ATDD tests that only verify mock interactions
✅ **Do** test real implementation with mocked external dependencies

### Anti-Pattern 8.2: Implementation-Focused Scenarios
❌ **Don't** write scenarios that describe technical implementation details
✅ **Do** write scenarios that describe business value and user outcomes

### Anti-Pattern 8.3: Monolithic Feature Files
❌ **Don't** put all scenarios in a single massive feature file
✅ **Do** organize scenarios by business capability and user journey

### Anti-Pattern 8.4: Brittle Step Definitions
❌ **Don't** write step definitions that break with minor UI or API changes
✅ **Do** write robust step definitions that focus on behavior, not implementation

---

## 9. Example Implementation Checklist

When implementing a new feature with ATDD:

- [ ] **User Story Defined**: Clear US-XXX identifier with acceptance criteria
- [ ] **Feature File Created**: Gherkin scenarios describing desired behavior
- [ ] **Step Definitions Implemented**: Real implementation calls with mocked externals
- [ ] **Local Tests Pass**: All scenarios pass in development environment
- [ ] **CI/CD Tests Pass**: All scenarios pass in automated pipeline
- [ ] **Traceability Updated**: Matrix updated with new mappings
- [ ] **Documentation Current**: Feature files reflect actual implementation
- [ ] **Coverage Verified**: Happy path, error cases, and edge cases covered

---

## 10. Tool Configuration

### Behave Configuration (`.behaverc`)
```ini
[behave]
paths = tests/features
show_timings = true
show_skipped = false
format = pretty
logging_level = INFO
```

### Environment Setup (`tests/features/environment.py`)
```python
def before_all(context):
    """Set up test environment before all scenarios."""
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)

def after_scenario(context, scenario):
    """Clean up after each scenario."""
    if hasattr(context, 'env_patcher'):
        context.env_patcher.stop()
```

---

## Summary

ATDD-driven AI development transforms **acceptance criteria into executable specifications** that guide implementation. By placing behavioral requirements at the center of development and using AI as an implementation partner, teams can:

- **Ensure software meets user needs** through specification-first development
- **Maintain high quality** through automated acceptance testing
- **Accelerate development** through AI-assisted implementation
- **Improve collaboration** through living documentation

Remember: **The test is the specification, and the specification is the test.**

---

*Last updated: 2025-01-06*
*Based on: [ATDD-driven AI development methodology](https://www.paulmduvall.com/atdd-driven-ai-development-how-prompting-and-tests-steer-the-code/)*